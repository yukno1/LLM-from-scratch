{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5c9d3b",
   "metadata": {},
   "source": [
    "## 5.1 Evaluating generative text models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689c017",
   "metadata": {},
   "source": [
    "### 5.1.1 Using GPT to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db62dd",
   "metadata": {},
   "source": [
    "GPT-2 used tensorflow, but tensorflow is not so popular nowadays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e360fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import GPTModel\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a171e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e20a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval(); # eval disables all random numbers\n",
    "# e.g., dropout, don't want random behaviour during inference\n",
    "# semicomma disables output\n",
    "# opposite of eval is train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba24c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from modules import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d19b888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 3626, 6100,  345]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = text_to_token_ids(start_context, tokenizer)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360ac0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f80eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4165b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6109,  3626,  6100,   345, 37532, 24086, 47843, 30961, 42348, 15635,\n",
       "         24110, 43231, 30967, 42175]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids # 4 input + 10 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98b8ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892a639",
   "metadata": {},
   "source": [
    "Before training, need a numeric way of visualization\\\n",
    "How to quantify?\\\n",
    "How to meassure how good it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e940f36",
   "metadata": {},
   "source": [
    "### 5.1.2 Calculating the text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de037fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ca194",
   "metadata": {},
   "source": [
    "target is shifted by 1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6382b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dec4cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02477c74",
   "metadata": {},
   "source": [
    "2 - rows; 3 - tokens each; 50257 - vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f3f527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27a65eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb0c7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Gathering SerbianFriday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a0e3bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([2.3466e-05, 2.0531e-05, 1.1733e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65dbdd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 2: tensor([4.2794e-05, 1.6248e-05, 1.1586e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283643c2",
   "metadata": {},
   "source": [
    "ouput of probabilities\\\n",
    "ideally, want to train them to approach 90%, i.e., model learns to associate generated probability corresponding to text targets to be very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e4d8207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2))) # concate so it's easy to apply same computation\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae1c60",
   "metadata": {},
   "source": [
    "The final cross-entropy loss (negative log probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fab0859e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8765)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*torch.mean(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5a13ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9039fa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 50257])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "logits_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2664c6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_flat = targets.flatten()\n",
    "targets_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "317d77e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8765)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e9d75",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dcb9862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52918.7773)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a50c8",
   "metadata": {},
   "source": [
    "describe how unsure;\\\n",
    "can be understood as the effective vocabulary size that the model is uncertain about at each step (in example, model is sure about no word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdcd68",
   "metadata": {},
   "source": [
    "### 5.1.3 Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cdc978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d03017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4efe6",
   "metadata": {},
   "source": [
    "Now divide it into a training set and a small validation set (evaluation and check if overfitting);\\\n",
    "Then divide set into boxes with size of context_length;\n",
    "then subdivide boxes into batches, batches are shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7940a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import create_dataloader_v1\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch02 import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90 # 10% for validation\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f891c9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    pass\n",
    "print(x.shape, y.shape) # to make sure no error\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    pass\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38efbcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380137d4",
   "metadata": {},
   "source": [
    "last 25 tokens are dropped\\\n",
    "each batch is of same size so won't have weird loss spikes during trining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4160746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c865b367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0f033de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.988502078586155\n",
      "Validation loss: 10.99034309387207\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be842a54",
   "metadata": {},
   "source": [
    "## 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f989515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device) # encode sample to token ids\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer) # translate to text\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e01f60bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2208411",
   "metadata": {},
   "source": [
    "In reality, when training a real new network, like Gemma3, deepseek, they train on 14, 15 trillion tokens\\\n",
    "since the novel is short, 1 epoch is not enough to learn\\\n",
    "normally train 1 or 1.5 epoch max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31362186",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      2\u001b[39m train_losses, val_losses, tokens_seen = train_model_simple(\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mmodel\u001b[49m, train_loader, val_loader, optimizer, device,\n\u001b[32m      4\u001b[39m     num_epochs=num_epochs, eval_freq=\u001b[32m5\u001b[39m, eval_iter=\u001b[32m5\u001b[39m,\n\u001b[32m      5\u001b[39m     start_context=\u001b[33m\"\u001b[39m\u001b[33mEvery effort moves you\u001b[39m\u001b[33m\"\u001b[39m, tokenizer=tokenizer\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc9578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
